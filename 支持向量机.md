# 支持向量机

## 线性模型

### 训练数据及标签

训练样本：**(x,y)**

* **x**为向量，可包含多个维度，即x=[x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub>,……]
* **y**为标签

### 超平面线性方程

$$
\omega^Tx+b=0
$$

* **$\omega$**为**待定向量，与x维度相同，$\omega$<sup>T</sup>为转置**
* **b**为**待定常数**

**学习过程即为通过(x,y)求解($\omega$,b)**

### 训练集线性可分

**{(x<sub>i</sub>,y<sub>i</sub>)}<sub>i=1~N</sub>**

存在**($\omega$,b)**，使：

对**所有i=1~N**，有：

* **若y<sub>i</sub>=+1,$\omega$<sup>T</sup>x+b>=0**,即**y<sub>i</sub>*($\omega$<sup>T</sup>x+b)>=0**
* **若y<sub>i</sub>=-1,$\omega$<sup>T</sup>x+b<0**

### 优化问题（二次规划）

* 最小化：**$\frac{1}{2}||\omega||^2$**
* 限制条件：**y<sub>i</sub>*($\omega$<sup>T</sup>x+b)>=1，i=1~N**，**y只取+-1**

点**(x<sub>0</sub>,y<sub>0</sub>)**面**$\omega$<sub>1</sub>x+$\omega$<sub>2</sub>x+b=0**距离公式
$$
d=\frac{|\omega_1x_0+\omega_2y_0+b|}{\sqrt{\omega_1^2+\omega_2^2}}
$$
向量**x<sub>0</sub>**超平面**$\omega$<sup>T</sup>x+b=0**距离公式
$$
\frac{|\omega^Tx_0+b|}{||\omega||}
$$

$$
||\omega||=\sqrt{\omega_1^2+\omega_2^2+\omega_3^2+\cdots}
$$

可以使用**a**缩放**($\omega$,b)**至**(a$\omega$,ab)**最终使在**支持向量x<sub>0</sub>**上有**|($\omega$<sup>T</sup>x_0+b)|=1**

此时，**支持向量**与**平面**的**距离**
$$
d=\frac{1}{||\omega||}
$$
最小化**||$\omega$||<sup>2</sup>**即为最大化**d**，**d**越大**容错**越大

查找极小值：**梯度下降**

## 非线性模型

### 优化处理

* 最小化：**$\frac{1}{2}||\omega||^2+C\sum_{i=1}^{N}{\xi_i}$**，$\xi_i$为**松弛变量**，**$C\sum_{i=1}^{N}{\xi_i}$**又称**正则项**，**C**为**预设参数**

* 限制条件：

  **y<sub>i</sub>*($\omega$<sup>T</sup>x+b)>=1-$\xi_i$**

   **$\xi_i$>=0**，**i=1~N**

### 高维映射

**$\psi$(x)**

**x(低维矢量)——>$\psi$(x)（高维矢量）**

例：

x<sub>1</sub>=[0,0]，x<sub>2</sub>=[1,1] $\in$ C<sub>1</sub>

x<sub>3</sub>=[0,1]，x<sub>4</sub>=[1,0] $\in$ C<sub>2</sub>

x=[a,b]——>$\psi$(x)=[a<sup>2</sup>,b<sup>2</sup>,a,b,ab]

$\psi$(x<sub>1</sub>)=[0,0,0,0,0]，$\psi$(x<sub>2</sub>)=[1,1,1,1,1]，$\psi$(x<sub>3</sub>)=[0,1,0,1,0]，$\psi$(x<sub>4</sub>)=[1,0,1,0,0]

此时令**$\omega$**=[-1,-1,-1,-1,6]，b=1，通过$\omega$<sup>T</sup>x+b即可分开

**维度越高，被线性分开的概率越大**

**$\psi$(x)**是**无限维**，可以不知道**$\psi$(x)**映射的**显示表达式**，仅需知道一个**核函数**，则该表达式依然可解
$$
K(x_1,x_2)=\psi(x_1)^T\psi(x_2)
$$

## 核函数

* **高斯核**：**$K(x_1,x_2)=e^{-\frac{||x_1-x_2||^2}{2\delta^2}}$**
* **多项式核**：**$K(x_1,x_2)=(x_1^Tx_2+1)^d$**，**d**为多项式的阶数
* **线性核**：**$K(x_1,x_2)=x_1^Tx_2$**
* **Tanh核**：**$K(x_1,x_2)=tanh(\beta~x_1^Tx_2+b)      tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$**

**$K(x_1,x_2)$**能写成**$\psi(x_1)^T\psi(x_2)$**的**充要条件**

* **$K(x_1,x_2)=K(x_2,x_1)$**（交换性）
* 任意**C<sub>i</sub>**（常数）、**x<sub>i</sub>**，**i=1~N**，有**$\sum_{i=1}^{N}\sum_{j=1}^{N}{C_i}{C_j}K(x_i,x_j)$>=0**（半正定性）

## 原问题与对偶问题

**原问题：**

最小化：f($\omega$)

限制条件：

* $g_i(\omega)$<=0，i=1~k
* $h_i(\omega)$=0，i=1~m

**对偶问题：** 

定义：L($\omega$,$\alpha$,$\beta$)=$f(\omega)+\sum_{i=1}^k\alpha_i~g_i(\omega)+\sum_{i=1}^m\beta_i~h_i(\omega)$=$f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)$

* **$\alpha$**维度与限制条件中**g($\omega$)**个数**k**一致
* **$\beta$**维度与限制条件中**h($\omega$)**个数**m**一致

**最大化：$\vartheta$($\alpha$,$\beta$)=inf{L($\omega$,$\alpha$,$\beta$)}，（inf即下界）**

**限制条件：$\alpha$<sub>i</sub>=0，i=1~k**

**如果$\omega^*$是原问题的解，$\alpha$、$\beta^*$是对偶问题的解，则有$f(\omega^*)$>=$\vartheta$($\alpha$,$\beta$)**

定义：**G=$f(\omega^*)$-$\vartheta$($\alpha$,$\beta$)**

**G**称为原问题与对偶问题的**间距**

对于某些特定的优化问题，可以证明**G=0**

**强对偶定理**：若**f($\omega$)**为**凸函数**，且**g($\omega$)=a$\omega$+b**，**h($\omega$)=c$\omega$+d**，**G=0**，即**$f(\omega^*)$=$\vartheta$($\alpha$,$\beta$)**

此时，对于**任意i=1~k**，**$a^*_i$=0**，或**$g_i(\omega^*)$=0**（K\~K\~T）

## 支持向量机中问题的转化

**原问题：**

最小化：**$\frac{1}{2}||\omega||^2+C\sum_{i=1}^{N}{\xi_i}$**

限制条件：

1. **y<sub>i</sub>*($\omega$<sup>T</sup>$\psi(x_i)$+b)>=1+$\xi_i$**

2. $\xi_i$<=0

**限制条件1**转化为**1+$\xi_i$-y<sub>i</sub>$\omega$<sup>T</sup>$\psi(x_i)$-y<sub>i</sub>b<=0**

**i=1~N**

**对偶问题：**

最大化：**$\vartheta$(m,n)=inf{L($\omega$,$\xi_i$,b)}=inf{$\frac{1}{2}||\omega||^2-C\sum_{i=1}^Nm_i~\xi_i+\sum_{i=1}^Nn_i~[1+\xi_i-y_i\omega^T\psi(x_i)-y_ib]$}**

限制条件：**m<sub>i</sub>=0，n<sub>i</sub>>=0**

**向量求导：$\omega$=[$\omega_1$,$\omega_2$,……]，f($\omega$)的导数为[f($\omega_1$),f($\omega_2$),……)]**

**f'($\omega$)=$\omega$**

**若f($\omega$)=$\omega^T$x，则f'($\omega$)=x**

**求极小值**

* **$\frac{dL}{d\omega}=0$——>$\omega=\sum_{i=1}^Nn_i~y_i\psi(x_i)$**

* **$\frac{dL}{d\xi_i}=0$——>$m_i+n_i=C$**

* **$\frac{dL}{db}=0$——>$\sum_{i=1}^Nn_i~y_i$=0**

代入$\vartheta$(m,n)

即

**最大化：$\vartheta$(n)=$\sum_{i=1}^N{n_i}-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Nn_in_jy_iy_jK(x_i,x_j)$**

**限制条件：**

* **0<=n<sub>i</sub><=C**
* **$\sum_{i=1}^Nn_iy_i=0$**

同时

**$\omega^T\psi(x)$=$\sum_{i=1}^Nn_iy_iK(x_i,y_i)$**

由**(K\~K\~T)**可得，对**任意i=1~N**，有

* **m<sub>i</sub>=0，或$\xi_i$=0**
* **n<sub>i</sub>=0，或$1+\xi_w-y_i\omega^T\psi(x_i)-y_ib$=0**

取一个**0<n<sub>i</sub><C——>m<sub>i</sub>=C-n<sub>i</sub>>0**

此时

* **m<sub>i</sub>!=0，$\xi_i$=0**
* **n<sub>i</sub>!=0，$1+\xi_i-y_i\omega^T\psi(x_i)-y_ib$=0**，即**$1-y_i\omega^T\psi(x_i)-y_ib$=0**

**b=$\frac{1-y_i\omega^T\psi(x_i)}{y_i}$=$\frac{1-y_i\sum_{i=1}^Nn_iy_iK(x_i,y_i)}{y_i}$**

应用中可使用所有0<n<sub>i</sub><C算出对应的b后**取平均值**

## SVM算法

**训练流程**

输入**{(x<sub>i</sub>,y<sub>i</sub>)}<sub>i=1~N</sub>**

**解优化问题：**

**最大化：$\vartheta$(n)=$\sum_{i=1}^N{n_i}-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^Nn_in_jy_iy_jK(x_i,x_j)$**

**限制条件：**

* **0<=n<sub>i</sub><=C**
* **$\sum_{i=1}^Nn_iy_i=0$**

**（SMO算法）**

**C**与$\gamma$=$\frac{1}{2\delta^2}$的取值：**暴力搜索**，**[$2^{-5}$,$2^{15}$]**及**[$2^{-15}$,$2^{3}$]**

## 归一化

设最小值为0，最大值为1

**x<sub>i-new</sub>=$\frac{x_i-avg(x)}{\delta^2}$**

~~~python
data[i] = (data[i]-data[i].mean())/data[i].std()
~~~

